{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Another binomial example </h1>\n",
    "<p>\n",
    "Suppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land where W indicates water and L indicates land. So in this example you observe six W (water) observations and three L (land) observations. Call this sequence of observations the data.</p>\n",
    "<p>\n",
    "    \n",
    "    WLWWWLWLW\n",
    "    \n",
    "    \n",
    "To get the logic moving, we need to make assumptions, and these assumptions constitute the model. Designing a simple Bayesian model benefits from a design loop with three steps.</p>\n",
    "\n",
    "<b>(1)</b> Data story: Motivate the model by narrating how the data might arise.\n",
    "\n",
    "<b>(2)</b> Update: Educate your model by feeding it the data.\n",
    "\n",
    "<b>(3)</b> Evaluate: All statistical models require supervision, leading possibly to model revision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> (1) Data Story </h1>\n",
    "<p> You can motivate your data story by trying to explain how each piece of data is born. This usually means describing aspects of the underlying reality as well as the sampling process. The data story in this case is simply a restatement of the sampling process:</p>\n",
    "\n",
    "(1) The true proportion of water covering the globe is p.\n",
    "\n",
    "(2) A single toss of the globe has a probability <b>p</b> of producing a water(W)observation.\n",
    "\n",
    "It has a probability<b> 1 − p</b> of producing a land (L) observation.\n",
    "\n",
    "\n",
    "(3) Each toss of the globe is independent of the others.\n",
    "The data story is then translated into a formal probability model. \n",
    "\n",
    "This probability model is easy to build, because the construction process can be usefully broken down into a series of component decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>Bayesian updating </h1>. \n",
    " <p>Our problem is one of using the evidence—the sequence of globe tosses—to decide among different possible proportions of water on the globe.  Each possible proportion may be more or less plausible, given the evidence. A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning, called Bayesian updating. </p>\n",
    " <b>\n",
    " For the sake of the example only, let’s program our Bayesian machine to initially assign the same plausibility to every proportion of water, every value of p. </b>\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_grid_approx(grid_points=5, success=4, tosses=5):\n",
    "    # define grid\n",
    "    p_grid = np.linspace(0, 1, grid_points) # |..|..|..|..|\n",
    "\n",
    "    # define prior\n",
    "    prior = np.repeat(5, grid_points)  # uniform Repeat elements of an array.\n",
    "    # prior = (p_grid >= 0.5).astype(int)  # truncated\n",
    "    # prior = np.exp(- 5 * abs(p_grid - 0.5))  # double exp\n",
    "\n",
    "    # compute likelihood at each point in the grid\n",
    "    likelihood = stats.binom.pmf(success, tosses, p_grid)\n",
    "\n",
    "    # compute product of likelihood and prior\n",
    "    unstd_posterior = likelihood * prior\n",
    "\n",
    "    # standardize the posterior, so it sums to 1\n",
    "    posterior = unstd_posterior / unstd_posterior.sum()\n",
    "    return p_grid, posterior \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11cea89d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# just firt observation \n",
    "idx=0      \n",
    "p_grid, posterior = posterior_grid_approx(100, 0, 0)\n",
    "ax[idx].plot(p_grid, posterior, \"o-\", label=f\"success =0\\ntosses = 0\")\n",
    "ax[idx].set_xlabel(\"fraction of water\")\n",
    "ax[idx].set_ylabel(\"posterior probability \\n of observing that fraction\")\n",
    "ax[idx].set_title(\"0 observation=prior distribution\")\n",
    "ax[idx].legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat for the  sequence WLWWWLWLW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Prior as probability distribution</b>. You could write the prior in the example here as: Pr(p)= 1 =1/1−0\n",
    "The prior is a probability distribution for the parameter.\n",
    "\n",
    "In general, for a uniform prior from a to b, the\n",
    "probability of any point in the interval is 1/(b − a).\n",
    "If you’re bothered by the fact that the probability of every value of p is 1, remember that every probability distribution must sum (integrate) to 1. The expression 1/(b − a) ensures that the area under the flat line from a to b is equal to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Model </h1>\n",
    "With all the above work, we can now summarize out model. The observed variables W and L are given relative counts through the binomial distribution. So we can write, as a shortcut:\n",
    "\n",
    "W ∼ Binomial(N, p)\n",
    "\n",
    "where N = W + L. The above is just a convention for communicating the assumption that the relative counts of ways to realize W in N trials with probability p on each trial comes from the binomial distribution. And the unobserved parameter p similarly gets:\n",
    "\n",
    "p ∼ Uniform(0, 1)\n",
    "\n",
    "This means that p has a uniform—flat—prior over its entire possible range, from zero to one. As I mentioned earlier, this is obviously not the best we could do, since we know the Earth has more water than land, even if we do not know the exact proportion yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> sampling the posterior</h1>\n",
    "<p>The posterior distribution is a probability distribution. And like all probability distributions, we can imagine drawing samples from it. The sampled events in this case are parameter values. Most parameters have no exact empirical realization. </p>\n",
    "\n",
    "<h1> Working with samples</h1>\n",
    "Working with samples transforms a problem in calculus into a problem in data summary, into a frequency format problem. An integral in a typical Bayesian context is just the total probability in some interval. That can be a challenging calculus problem. But once you have samples from the probability distribution, it’s just a matter of counting values in the interval. Even seemingly simple calculations, like confidence intervals, are made difficult once a model has many parameters. In those cases, one must average over the uncertainty in all other parameters, when describing the uncertainty in a focal parameter. This requires a complicated integral, but only a very simple data summary. An empirical attack on the posterior allows the scientist to ask and answer more questions about the model, without relying upon a captive mathematician. For this reason, it is often easier and more intuitive to work with samples from the posterior, than to work with probabilities and integrals directly. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, a strong minority current of modern statistical theory offers the possibility of avoiding both the <b>magic and assumptions of </b>classical statistical theory through randomization techniques known collectively as resampling.  <p>These techniques take a given sample and either create new sam- ples by randomly selecting values from the given sample with <b>replacement</b>, or by randomly shuffling labels on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we wish to draw 10,000 samples from this posterior. \n",
    "    \n",
    "    Imagine the posterior is a bucket full of parameter values, numbers such as 0.1, 0.7, 0.5, 1, etc. \n",
    "    \n",
    "    Within the bucket, each value exists in proportion to its posterior probability, such that values near the peak are much more common than those in the tails. \n",
    "    \n",
    "    We’re going to scoop out 10,000 values from the bucket. \n",
    "    \n",
    "    Provided the bucket is well mixed, the resulting samples will have the same proportions as the exact posterior density. \n",
    "    \n",
    "    Therefore the individual values of p will appear in our samples in proportion to the posterior plausibility of each value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid, posterior = posterior_grid_approx(grid_points=100, success=6, tosses=9)\n",
    "samples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax0.plot(samples, \"o\", alpha=0.2)\n",
    "ax0.set_xlabel(\"sample number\")\n",
    "ax0.set_ylabel(\"proportion water (p)\")\n",
    "az.plot_kde(samples, ax=ax1)\n",
    "ax1.set_xlabel(\"proportion water (p)\")\n",
    "ax1.set_ylabel(\"density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can see that the estimated density is very similar to ideal posterior you computed via grid approximation. If you draw even more samples, maybe 1e5 or 1e6, the density estimate will get more and more similar to the ideal.\n",
    "</p>\n",
    "<p>\n",
    "All you’ve done so far is crudely replicate the posterior density you had already com- puted. That isn’t of much value. But next it is time to use these samples to describe and understand the posterior. That is of great value.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sampling to summarize </h1>\n",
    "<p>\n",
    "Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose. But common questions include:</p>\n",
    "\n",
    "• How much posterior probability lies below some parameter value?\n",
    "\n",
    "• How much posterior probability lies between two parameter values?\n",
    "\n",
    "• Which parameter value marks the lower 5% of the posterior probability?\n",
    "\n",
    "• Which range of parameter values contains 90% of the posterior probability? \n",
    "\n",
    "• Which parameter value has highest posterior probability?\n",
    "<p>\n",
    "These simple questions can be usefully divided into questions about (1) intervals of defined boundaries, (2) questions about intervals of defined probability mass, and (3) questions about point estimates. We’ll see how to approach these questions using samples from the posterior.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Intervals of defined boundaries </h1>. Suppose I ask you for the posterior probability that the proportion of water is less than 0.5. Using the grid-approximate posterior, you can just add up all of the probabilities, where the corresponding parameter value is less than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17183313110747478"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(posterior[p_grid < 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 17% of the posterior probability is below 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "So let’s see how to perform the same calculation, using samples from the posterior. This approach does generalize to complex models with many parameters, and so you can use it everywhere. All you have to do is similarly \n",
    "    \n",
    "    add up all of the samples below 0.5, \n",
    "    \n",
    "    but also divide the resulting count by the total number of samples. I\n",
    "    \n",
    "    In other words, find the frequency of parameter values below 0.5:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(samples < 0.5) / 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> how much posterior probability lies between 0.5 and 0.75:<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6102"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((samples > 0.5) & (samples < 0.75)) / 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 61% of the posterior probability lies between 0.5 and 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Intervals of defined mass.</h1> It is more common to see scientific journals reporting an interval of defined mass, usually known as a confidence interval.\n",
    "An interval of posterior probability, such as the ones we are working with, may instead be called a credible interval.\n",
    "\n",
    "\n",
    "We’re going to call it a compatibility interval instead, in order to avoid the unwarranted implications of “confidence” and “credibility. What the interval indicates is a range of parameter values compatible with the model and data. The model and data themselves may not inspire confidence, in which case the interval will not either.\n",
    "\n",
    "\n",
    "These posterior intervals report two parameter values that contain between them a specified amount of posterior probability, a probability mass. For this type of interval, it is easier to find the answer by using samples from the posterior than by using a grid approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose for example you want to know the boundaries of the lower 80% posterior probabil- ity. You know this interval starts at p = 0. To find out where it stops, think of the samples as data and ask where the 80th percentile lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575757575757577"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(samples, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The middle 80% interval lies between the 10th percentile and the 90th percentile. These boundaries are found using the same approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44444444, 0.80808081])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(samples, [10, 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intervals of this sort, which assign equal probability mass to each tail, are very common\n",
    "in the scientific literature. We’ll call them <b>percentile intervals </b>(PI). These intervals do a good job of communicating the shape of a distribution, as long as the distribution <b>isn’t  too asymmetrical</b>. But in terms of supporting inferences about which parameters are consistent with the data, they are not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the posterior distribution where you  observe three waters in three tosses and a uniform (flat) prior. It is highly skewed, having its maximum value at the boundary, p = 1. You can compute it, via grid approximation, with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_grid, posterior = posterior_grid_approx(grid_points=100,success=3, tosses=3)\n",
    "plt.plot(p_grid, posterior)\n",
    "plt.xlabel(\"proportion water (p)\")\n",
    "plt.ylabel(\"Density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50 % interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70707071, 0.92929293])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)\n",
    "np.percentile(samples, [25, 75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interval assigns 25% of the probability mass above and below the interval. So it pro- vides the central 50% probability. But in this example, it ends up excluding the most prob- able parameter values, near p = 1. So in terms of describing the shape of the posterior distribution—which is really all these intervals are asked to do—the percentile interval can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "probability mass. If you think about it, there must be an infinite number of posterior intervals with the same mass. But if you want an interval that best represents the parameter values most consistent with the data, then you want the densest of these intervals. That’s what the HPDI is:  <b>highest posterior density interval</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49494949, 1.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.hpd(samples, credible_interval=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HPDI also has some disadvantages. HPDI is more computationally intensive than PI and suffers from greater simulation variance, which is a fancy way of saying that it is sensitive to how many samples you draw from the posterior. It is also harder to understand and many scientific audiences will not appreciate its features, while they will immediately understand a percentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly) as percentile intervals \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Overall, if the choice of interval type makes a big difference, then you shouldn’t be using intervals to summarize the posterior. Remember, the entire posterior distribution is the Bayesian “estimate.” It summarizes the relative plausibilities of each possible value of the parameter. Intervals of the distribution are just helpful for summarizing it. If choice of in- terval leads to different inferences, then you’d be better off just plotting the entire posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Point estimates</h1>. \n",
    "<h2>a maximum a posteriori (MAP) estimate </h2>\n",
    "The third and final common summary task for the posterior is to produce point estimates of some kind. Given the entire posterior distribution, what value should you report? This seems like an innocent question, but it is difficult to answer. The Bayesian parameter estimate is precisely the entire posterior distribution, which is not a sin- gle number, but instead a function that maps each unique parameter value onto a plausibility value. So really the most important thing to note is that you don’t have to choose a point es- timate. It’s hardly ever necessary and often harmful. It discards information.\n",
    "But if you must produce a single point to summarize the posterior, you’ll have to ask and answer more questions. Consider the following example. Suppose again the globe tossing experiment in which we observe 3 waters out of 3 tosses;\n",
    "Let’s consider three alternative point estimates. First, it is very common for scientists to report the parameter value with highest posterior probability, <b> a maximum a posteriori (MAP) estimate </b>. You can easily compute the MAP in this example (x)from grid appro:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_grid[posterior == max(posterior)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you instead have <b>samples from the posterior </b>, you can still approximate the same point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mode(samples)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> posterior mean and median </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8018868686868688, 0.8383838383838385)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(samples), np.median(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are also point estimates, and they also summarize the posterior. But all three—the mode (MAP), mean, and median—are different in this case.How can we choose among them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One principled way to go beyond using the entire posterior as the estimate is to choose a loss function. A loss function is a rule that tells you the cost associated with using any particular point estimate. While statisticians and game theorists have long been interested in loss functions, and how Bayesian inference supports them, scientists hardly ever use them explicitly. The key insight is that different loss functions imply different point estimates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here’s an example to help us work through the procedure. Suppose I offer you a bet. Tell me which value of p, the proportion of water on the Earth, you think is correct. I will pay you \\$100, if you get it exactly right. But I will subtract money from your gain, proportional to the distance of your decision from the correct value. Precisely, your loss is proportional to the absolute value of d − p, where d is your decision and p is the correct answer. We could change the precise dollar values involved, without changing the important aspects of this problem. What matters is that the loss is proportional to the distance of your decision from the true value.\n",
    "\n",
    "\n",
    "Now once you have the posterior distribution in hand, how should you use it to maxi-\n",
    "mize your expected winnings? It turns out that the parameter value that maximizes expected\n",
    "winnings (minimizes expected loss) is the median of the posterior distribution. Let’s calcu-\n",
    "late that fact, without using a mathematical proof. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating expected loss for any given decision means avereaging over our uncertainty in the true value. Of course we don’t know the true value, in most cases. But if we are going to use our model’s information about the parameter, that means using the entire posterior distribution. So suppose we decide p = 0.5 will be our decision. Then the expected loss will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31626874808692995"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( posterior*abs( 0.5 - p_grid ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbols posterior and p_grid are the same ones we’ve been using throughout this chapter, containing the posterior probabilities and the parameter values, respectively. All the code above does is compute the weighted average loss, where each loss is weighted by its corresponding posterior probability There’s a trick for repeating this calculation for every possible decision,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84848485])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = [sum(posterior * abs(p - p_grid)) for p in p_grid]\n",
    "p_grid[loss == min(loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we to learn from all of this? In order to decide upon a point estimate, a single-value summary of the posterior distribution, we need to pick a loss function. Different loss functions nominate different point estimates. The two most common examples are the absolute loss as above, which leads to the median as the point estimate, and the quadratic loss (d − p)2, which leads to the posterior mean (mean(samples)) as the point estimate. When the posterior distribution is symmetrical and normal-looking, then the median and mean converge to the same point, which relaxes some anxiety we might have about choosing a loss function. For the original globe tossing data (6 waters in 9 tosses), for example, the mean and median are barely different.\n",
    "\n",
    "\n",
    "In principle, though, the details of the applied context may demand a rather unique loss function. Consider a practical example like deciding whether or not to order an evacuation, based upon an estimate of hurricane wind speed. Damage to life and property increases very rapidly as wind speed increases. There are also costs to ordering an evacuation when none is needed, but these are much smaller. Therefore the implied loss function is highly asymmetric, rising sharply as true wind speed exceeds our guess, but rising only slowly as true wind speed falls below our guess. In this context, the optimal point estimate would tend to be larger than posterior mean or median. Moreover, the real issue is whether or not to order an evacuation, and so producing a point estimate of wind speed may not be necessary at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> sampling to simulate prediction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another common job for samples is to ease simulation of the model’s implied obser- vations. Generating implied observations from a model is useful for at least four distinct reasons.\n",
    "\n",
    "(1) Model design. We can sample not only from the posterior, but also from the prior. Seeing what the model expects, before the data arrive, is the best way to understand the implications of the prior. We’ll do a lot of this in later chapters, where there will be multiple parameters and so their joint implications are not always very clear.\n",
    "\n",
    "(2) Model checking. After a model is updated using data, it is worth simulating im- plied observations, to check both whether the fit worked correctly and to investi- gate model behavior.\n",
    "\n",
    "(3) Software validation. In order to be sure that our model fitting software is working, it helps to simulate observations under a known model and then attempt to recover the values of the parameters the data were simulated under.\n",
    "\n",
    "(4) Research design. If you can simulate observations from your hypothesis, then you can evaluate whether the research design can be effective. In a narrow sense, this means doing power analysis, but the possibilities are much broader.\n",
    "\n",
    "(5) Forecasting. Estimates can be used to simulate new predictions, for new cases and future observations. These forecasts can be useful as applied prediction, but also for model criticism and revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s summarize the globe tossing model that you’ve been working with for two chapters now. A fixed true proportion of water p exists, and that is the target of our inference. Tossing the globe in the air and catching it produces observations of “water” and “land” that appear in proportion to p and 1 − p, respectively.\n",
    "\n",
    "Now note that these assumptions not only allow us to infer the plausibility of each possi- ble value of p, after observation. That’s what you did in the previous chapter. These assump- tions also allow us to simulate the observations that the model implies. They allow this, because likelihood functions work in both directions. Given a realized observation, \n",
    "\n",
    "the like- lihood function says how plausible the observation is. \n",
    "\n",
    "And given only the parameters, \n",
    "\n",
    "the likelihood defines a distribution of possible observations that we can sample from, to simu-late observation. \n",
    "\n",
    "In this way, Bayesian models are always generative, capable of simulating predictions. Many non-Bayesian models are also generative, but many are not.\n",
    "We will call such simulated data dummy data, to indicate that it is a stand-in for actual data. With the globe tossing model, the dummy data arises from a binomial likelihood:\n",
    "Pr(W|N, p) =( N! /W!(N − W)!)  p^W (1 − p)^N−W \n",
    "\n",
    "\n",
    "here W is an observed count of “water” and N is the number of tosses. Suppose N = 2, two tosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water. You can quickly compute the probability of each, for any given value of p. Let’s use p = 0.7, which is just about the true proportion of water on the Earth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09, 0.42, 0.49])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.binom.pmf(range(3), n=2, p=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there’s a 9% chance of observing w = 0, a 42% chance of w = 1, and a 49% chance of w = 2. If you change the value of p, you’ll get a different distribution of implied observations.\n",
    "Now we’re going to simulate observations, using these probabilities. This is done by sampling from the distribution just described above.\n",
    "\n",
    "\n",
    "You could use resampling to do this, but phyton  provides convenient sampling functions for all the ordinary probability distributions, like the binomial. So a single dummy data observation of W can be sampled with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.binom.rvs(n=2, p=0.7, size=1) #That 1 means “1 water in 2 tosses.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.binom.rvs(n=2, p=0.7, size=10) #A set of 10 simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears in proportion to its likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09059, 0.42027, 0.48914]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_w = stats.binom.rvs(n=2, p=0.7, size=int(1e5))\n",
    "[(dummy_w == i).mean() for i in range(3)] #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two tosses of the globe isn’t much of a sample, though. So now let’s simulate the same sample size as before, 9 tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dummy_w = stats.binom.rvs(n=9, p=0.7, size=int(1e5))\n",
    "# dummy_w = stats.binom.rvs(n=9, p=0.6, size=int(1e4))\n",
    "dummy_w = stats.binom.rvs(n=9, p=samples)\n",
    "plt.hist(dummy_w, bins=50)\n",
    "plt.xlabel(\"dummy water count\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of the time the expected obser- vation does not contain water in its true proportion, 0.7. That’s the nature of observation: There is a one-to-many relationship between data and data-generating processes. You should experiment with sample size, the size input in the code above, as well as the prob, to see how the distribution of simulated samples changes shape and location."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
